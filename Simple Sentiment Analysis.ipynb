{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"Reddit_Data.csv\")\n",
    "y = pd.read_csv('Twitter_Data.csv')\n",
    "y = y.rename({'clean_text':'clean_comment'},axis=1)\n",
    "x = x.append(y, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['category'] = x['category'].replace({1:'Positive',0:'Neutral',-1:'Negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(x['clean_comment']).astype(str)\n",
    "category = np.array(x['category']).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(sentences)\n",
    "X_train = vect.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(MultinomialNB(), X_train, category, cv=6)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = np.array([input(\"Please enter a comment:\\n\")])\n",
    "print('That comment is ',clf.predict(vect.transform(comment)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, X, y):\n",
    "        'Initialization'\n",
    "        self.labels = torch.tensor(y)\n",
    "        self.features = torch.tensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"Reddit_Data.csv\")\n",
    "y = pd.read_csv('Twitter_Data.csv')\n",
    "y = y.rename({'clean_text':'clean_comment'},axis=1)\n",
    "x = x.append(y, ignore_index=True).dropna()\n",
    "sentences = np.array(x['clean_comment']).astype(str)\n",
    "category = np.array(x['category'].astype('category').cat.codes).astype('int8')\n",
    "vect = CountVectorizer().fit(sentences)\n",
    "X_train_vect = vect.transform(sentences)\n",
    "tf = TfidfTransformer().fit(X_train_vect)\n",
    "X_train = tf.transform(X_train_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_vect.astype('int8').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your answer goes here\n",
    "X_training, X_test, y_train, y_test  = train_test_split(X_train, category, test_size=0.2)\n",
    "training_set = Dataset(X_training, y_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set,batch_size=256)\n",
    "testing_set = Dataset(X_test, y_test)\n",
    "testing_generator = torch.utils.data.DataLoader(testing_set,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Any Pytorch's network class is an extension of the torch.nn.Module parent class.\n",
    "# To define a network class, you need to define at least 2 methods: an __init__() method (constructor) and a forward() method\n",
    "class SimpleNetwork(nn.Module):\n",
    "    # Create the network class by filling in this block of code\n",
    "\n",
    "    # Create the constructor. Add any additional arguments as you wish\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNetwork,self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.reLu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size , num_classes)\n",
    "    # Define the feed forward function.\n",
    "    # x is the input example/examples.\n",
    "    # Add any additional arguments as you wish.\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.reLu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/ 100\n",
      "Time Taken:  100.6377522945404\n",
      "Training set: Average loss: 0.0041, Accuracy: 71953/160094 (45%)\n",
      "\n",
      "Test set: Average loss: 0.0160, Accuracy: 20123/40024 (50%)\n",
      "\n",
      "epoch 2/ 100\n",
      "Time Taken:  101.31757545471191\n",
      "Training set: Average loss: 0.0039, Accuracy: 84833/160094 (53%)\n",
      "\n",
      "Test set: Average loss: 0.0154, Accuracy: 21941/40024 (55%)\n",
      "\n",
      "epoch 3/ 100\n",
      "Time Taken:  102.95938086509705\n",
      "Training set: Average loss: 0.0038, Accuracy: 89552/160094 (56%)\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 22717/40024 (57%)\n",
      "\n",
      "epoch 4/ 100\n",
      "Time Taken:  103.15021824836731\n",
      "Training set: Average loss: 0.0036, Accuracy: 92878/160094 (58%)\n",
      "\n",
      "Test set: Average loss: 0.0143, Accuracy: 23582/40024 (59%)\n",
      "\n",
      "epoch 5/ 100\n",
      "Time Taken:  96.81922721862793\n",
      "Training set: Average loss: 0.0035, Accuracy: 96938/160094 (61%)\n",
      "\n",
      "Test set: Average loss: 0.0137, Accuracy: 24678/40024 (62%)\n",
      "\n",
      "epoch 6/ 100\n",
      "Time Taken:  95.92038130760193\n",
      "Training set: Average loss: 0.0033, Accuracy: 101969/160094 (64%)\n",
      "\n",
      "Test set: Average loss: 0.0130, Accuracy: 25989/40024 (65%)\n",
      "\n",
      "epoch 7/ 100\n",
      "Time Taken:  96.62834525108337\n",
      "Training set: Average loss: 0.0032, Accuracy: 107475/160094 (67%)\n",
      "\n",
      "Test set: Average loss: 0.0124, Accuracy: 27185/40024 (68%)\n",
      "\n",
      "epoch 8/ 100\n",
      "Time Taken:  96.19160914421082\n",
      "Training set: Average loss: 0.0030, Accuracy: 111938/160094 (70%)\n",
      "\n",
      "Test set: Average loss: 0.0118, Accuracy: 28237/40024 (71%)\n",
      "\n",
      "epoch 9/ 100\n",
      "Time Taken:  97.32571601867676\n",
      "Training set: Average loss: 0.0028, Accuracy: 115828/160094 (72%)\n",
      "\n",
      "Test set: Average loss: 0.0112, Accuracy: 29213/40024 (73%)\n",
      "\n",
      "epoch 10/ 100\n",
      "Time Taken:  99.84825587272644\n",
      "Training set: Average loss: 0.0027, Accuracy: 119001/160094 (74%)\n",
      "\n",
      "Test set: Average loss: 0.0107, Accuracy: 29976/40024 (75%)\n",
      "\n",
      "epoch 11/ 100\n",
      "Time Taken:  101.69659686088562\n",
      "Training set: Average loss: 0.0026, Accuracy: 121618/160094 (76%)\n",
      "\n",
      "Test set: Average loss: 0.0102, Accuracy: 30595/40024 (76%)\n",
      "\n",
      "epoch 12/ 100\n",
      "Time Taken:  100.50343084335327\n",
      "Training set: Average loss: 0.0025, Accuracy: 123959/160094 (77%)\n",
      "\n",
      "Test set: Average loss: 0.0097, Accuracy: 31083/40024 (78%)\n",
      "\n",
      "epoch 13/ 100\n",
      "Time Taken:  99.81525444984436\n",
      "Training set: Average loss: 0.0023, Accuracy: 125975/160094 (79%)\n",
      "\n",
      "Test set: Average loss: 0.0093, Accuracy: 31556/40024 (79%)\n",
      "\n",
      "epoch 14/ 100\n",
      "Time Taken:  102.87923288345337\n",
      "Training set: Average loss: 0.0022, Accuracy: 127853/160094 (80%)\n",
      "\n",
      "Test set: Average loss: 0.0089, Accuracy: 31996/40024 (80%)\n",
      "\n",
      "epoch 15/ 100\n",
      "Time Taken:  98.26176404953003\n",
      "Training set: Average loss: 0.0021, Accuracy: 129578/160094 (81%)\n",
      "\n",
      "Test set: Average loss: 0.0086, Accuracy: 32384/40024 (81%)\n",
      "\n",
      "epoch 16/ 100\n",
      "Time Taken:  100.23467683792114\n",
      "Training set: Average loss: 0.0021, Accuracy: 131246/160094 (82%)\n",
      "\n",
      "Test set: Average loss: 0.0083, Accuracy: 32744/40024 (82%)\n",
      "\n",
      "epoch 17/ 100\n",
      "Time Taken:  98.1101508140564\n",
      "Training set: Average loss: 0.0020, Accuracy: 132712/160094 (83%)\n",
      "\n",
      "Test set: Average loss: 0.0080, Accuracy: 33041/40024 (83%)\n",
      "\n",
      "epoch 18/ 100\n",
      "Time Taken:  98.77176642417908\n",
      "Training set: Average loss: 0.0019, Accuracy: 133979/160094 (84%)\n",
      "\n",
      "Test set: Average loss: 0.0077, Accuracy: 33298/40024 (83%)\n",
      "\n",
      "epoch 19/ 100\n",
      "Time Taken:  98.75606989860535\n",
      "Training set: Average loss: 0.0018, Accuracy: 135268/160094 (84%)\n",
      "\n",
      "Test set: Average loss: 0.0074, Accuracy: 33545/40024 (84%)\n",
      "\n",
      "epoch 20/ 100\n",
      "Time Taken:  97.07923436164856\n",
      "Training set: Average loss: 0.0018, Accuracy: 136405/160094 (85%)\n",
      "\n",
      "Test set: Average loss: 0.0072, Accuracy: 33795/40024 (84%)\n",
      "\n",
      "epoch 21/ 100\n",
      "Time Taken:  100.95201516151428\n",
      "Training set: Average loss: 0.0017, Accuracy: 137465/160094 (86%)\n",
      "\n",
      "Test set: Average loss: 0.0070, Accuracy: 34054/40024 (85%)\n",
      "\n",
      "epoch 22/ 100\n",
      "Time Taken:  101.77030205726624\n",
      "Training set: Average loss: 0.0016, Accuracy: 138443/160094 (86%)\n",
      "\n",
      "Test set: Average loss: 0.0068, Accuracy: 34264/40024 (86%)\n",
      "\n",
      "epoch 23/ 100\n",
      "Time Taken:  102.43228960037231\n",
      "Training set: Average loss: 0.0016, Accuracy: 139310/160094 (87%)\n",
      "\n",
      "Test set: Average loss: 0.0066, Accuracy: 34465/40024 (86%)\n",
      "\n",
      "epoch 24/ 100\n",
      "Time Taken:  101.90758156776428\n",
      "Training set: Average loss: 0.0015, Accuracy: 140157/160094 (88%)\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 34658/40024 (87%)\n",
      "\n",
      "epoch 25/ 100\n",
      "Time Taken:  98.99005484580994\n",
      "Training set: Average loss: 0.0015, Accuracy: 140839/160094 (88%)\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 34831/40024 (87%)\n",
      "\n",
      "epoch 26/ 100\n",
      "Time Taken:  99.54956316947937\n",
      "Training set: Average loss: 0.0015, Accuracy: 141592/160094 (88%)\n",
      "\n",
      "Test set: Average loss: 0.0061, Accuracy: 35006/40024 (87%)\n",
      "\n",
      "epoch 27/ 100\n",
      "Time Taken:  99.7527482509613\n",
      "Training set: Average loss: 0.0014, Accuracy: 142232/160094 (89%)\n",
      "\n",
      "Test set: Average loss: 0.0060, Accuracy: 35150/40024 (88%)\n",
      "\n",
      "epoch 28/ 100\n",
      "Time Taken:  99.38040947914124\n",
      "Training set: Average loss: 0.0014, Accuracy: 142843/160094 (89%)\n",
      "\n",
      "Test set: Average loss: 0.0059, Accuracy: 35306/40024 (88%)\n",
      "\n",
      "epoch 29/ 100\n",
      "Time Taken:  99.20424890518188\n",
      "Training set: Average loss: 0.0013, Accuracy: 143423/160094 (90%)\n",
      "\n",
      "Test set: Average loss: 0.0057, Accuracy: 35440/40024 (89%)\n",
      "\n",
      "epoch 30/ 100\n",
      "Time Taken:  98.71680545806885\n",
      "Training set: Average loss: 0.0013, Accuracy: 143968/160094 (90%)\n",
      "\n",
      "Test set: Average loss: 0.0056, Accuracy: 35568/40024 (89%)\n",
      "\n",
      "epoch 31/ 100\n",
      "Time Taken:  98.6327292919159\n",
      "Training set: Average loss: 0.0013, Accuracy: 144474/160094 (90%)\n",
      "\n",
      "Test set: Average loss: 0.0055, Accuracy: 35665/40024 (89%)\n",
      "\n",
      "epoch 32/ 100\n",
      "Time Taken:  99.69969987869263\n",
      "Training set: Average loss: 0.0012, Accuracy: 144947/160094 (91%)\n",
      "\n",
      "Test set: Average loss: 0.0054, Accuracy: 35773/40024 (89%)\n",
      "\n",
      "epoch 33/ 100\n",
      "Time Taken:  99.6206283569336\n",
      "Training set: Average loss: 0.0012, Accuracy: 145401/160094 (91%)\n",
      "\n",
      "Test set: Average loss: 0.0053, Accuracy: 35864/40024 (90%)\n",
      "\n",
      "epoch 34/ 100\n",
      "Time Taken:  96.2995491027832\n",
      "Training set: Average loss: 0.0012, Accuracy: 145850/160094 (91%)\n",
      "\n",
      "Test set: Average loss: 0.0052, Accuracy: 35967/40024 (90%)\n",
      "\n",
      "epoch 35/ 100\n",
      "Time Taken:  95.72231364250183\n",
      "Training set: Average loss: 0.0012, Accuracy: 146267/160094 (91%)\n",
      "\n",
      "Test set: Average loss: 0.0051, Accuracy: 36064/40024 (90%)\n",
      "\n",
      "epoch 36/ 100\n",
      "Time Taken:  95.25208830833435\n",
      "Training set: Average loss: 0.0011, Accuracy: 146673/160094 (92%)\n",
      "\n",
      "Test set: Average loss: 0.0051, Accuracy: 36139/40024 (90%)\n",
      "\n",
      "epoch 37/ 100\n",
      "Time Taken:  98.6204526424408\n",
      "Training set: Average loss: 0.0011, Accuracy: 147034/160094 (92%)\n",
      "\n",
      "Test set: Average loss: 0.0050, Accuracy: 36223/40024 (91%)\n",
      "\n",
      "epoch 38/ 100\n",
      "Time Taken:  98.81489491462708\n",
      "Training set: Average loss: 0.0011, Accuracy: 147370/160094 (92%)\n",
      "\n",
      "Test set: Average loss: 0.0049, Accuracy: 36304/40024 (91%)\n",
      "\n",
      "epoch 39/ 100\n",
      "Time Taken:  99.06512236595154\n",
      "Training set: Average loss: 0.0011, Accuracy: 147707/160094 (92%)\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 36378/40024 (91%)\n",
      "\n",
      "epoch 40/ 100\n",
      "Time Taken:  99.01908087730408\n",
      "Training set: Average loss: 0.0010, Accuracy: 148043/160094 (92%)\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 36448/40024 (91%)\n",
      "\n",
      "epoch 41/ 100\n",
      "Time Taken:  99.014075756073\n",
      "Training set: Average loss: 0.0010, Accuracy: 148364/160094 (93%)\n",
      "\n",
      "Test set: Average loss: 0.0047, Accuracy: 36518/40024 (91%)\n",
      "\n",
      "epoch 42/ 100\n",
      "Time Taken:  98.63473129272461\n",
      "Training set: Average loss: 0.0010, Accuracy: 148650/160094 (93%)\n",
      "\n",
      "Test set: Average loss: 0.0046, Accuracy: 36588/40024 (91%)\n",
      "\n",
      "epoch 43/ 100\n",
      "Time Taken:  99.60761666297913\n",
      "Training set: Average loss: 0.0010, Accuracy: 148915/160094 (93%)\n",
      "\n",
      "Test set: Average loss: 0.0046, Accuracy: 36636/40024 (92%)\n",
      "\n",
      "epoch 44/ 100\n",
      "Time Taken:  99.75775265693665\n",
      "Training set: Average loss: 0.0010, Accuracy: 149179/160094 (93%)\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 36704/40024 (92%)\n",
      "\n",
      "epoch 45/ 100\n",
      "Time Taken:  98.6577160358429\n",
      "Training set: Average loss: 0.0010, Accuracy: 149417/160094 (93%)\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 36754/40024 (92%)\n",
      "\n",
      "epoch 46/ 100\n",
      "Time Taken:  95.68138408660889\n",
      "Training set: Average loss: 0.0009, Accuracy: 149674/160094 (93%)\n",
      "\n",
      "Test set: Average loss: 0.0044, Accuracy: 36789/40024 (92%)\n",
      "\n",
      "epoch 47/ 100\n",
      "Time Taken:  95.76459836959839\n",
      "Training set: Average loss: 0.0009, Accuracy: 149903/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0043, Accuracy: 36824/40024 (92%)\n",
      "\n",
      "epoch 48/ 100\n",
      "Time Taken:  95.07677865028381\n",
      "Training set: Average loss: 0.0009, Accuracy: 150121/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0043, Accuracy: 36875/40024 (92%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49/ 100\n",
      "Time Taken:  96.74032878875732\n",
      "Training set: Average loss: 0.0009, Accuracy: 150342/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0042, Accuracy: 36909/40024 (92%)\n",
      "\n",
      "epoch 50/ 100\n",
      "Time Taken:  98.1703085899353\n",
      "Training set: Average loss: 0.0009, Accuracy: 150543/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0042, Accuracy: 36951/40024 (92%)\n",
      "\n",
      "epoch 51/ 100\n",
      "Time Taken:  99.6336395740509\n",
      "Training set: Average loss: 0.0009, Accuracy: 150735/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0042, Accuracy: 36994/40024 (92%)\n",
      "\n",
      "epoch 52/ 100\n",
      "Time Taken:  99.02408528327942\n",
      "Training set: Average loss: 0.0008, Accuracy: 150924/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0041, Accuracy: 37046/40024 (93%)\n",
      "\n",
      "epoch 53/ 100\n",
      "Time Taken:  99.5065245628357\n",
      "Training set: Average loss: 0.0008, Accuracy: 151093/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0041, Accuracy: 37085/40024 (93%)\n",
      "\n",
      "epoch 54/ 100\n",
      "Time Taken:  99.57158350944519\n",
      "Training set: Average loss: 0.0008, Accuracy: 151245/160094 (94%)\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 37123/40024 (93%)\n",
      "\n",
      "epoch 55/ 100\n",
      "Time Taken:  98.73882555961609\n",
      "Training set: Average loss: 0.0008, Accuracy: 151412/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 37150/40024 (93%)\n",
      "\n",
      "epoch 56/ 100\n",
      "Time Taken:  98.7211549282074\n",
      "Training set: Average loss: 0.0008, Accuracy: 151561/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0040, Accuracy: 37176/40024 (93%)\n",
      "\n",
      "epoch 57/ 100\n",
      "Time Taken:  98.75083661079407\n",
      "Training set: Average loss: 0.0008, Accuracy: 151707/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0039, Accuracy: 37203/40024 (93%)\n",
      "\n",
      "epoch 58/ 100\n",
      "Time Taken:  100.40434098243713\n",
      "Training set: Average loss: 0.0008, Accuracy: 151852/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0039, Accuracy: 37230/40024 (93%)\n",
      "\n",
      "epoch 59/ 100\n",
      "Time Taken:  99.87986350059509\n",
      "Training set: Average loss: 0.0008, Accuracy: 151997/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0039, Accuracy: 37261/40024 (93%)\n",
      "\n",
      "epoch 60/ 100\n",
      "Time Taken:  99.20124673843384\n",
      "Training set: Average loss: 0.0008, Accuracy: 152135/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0038, Accuracy: 37287/40024 (93%)\n",
      "\n",
      "epoch 61/ 100\n",
      "Time Taken:  98.72081065177917\n",
      "Training set: Average loss: 0.0007, Accuracy: 152261/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0038, Accuracy: 37306/40024 (93%)\n",
      "\n",
      "epoch 62/ 100\n",
      "Time Taken:  99.94291996955872\n",
      "Training set: Average loss: 0.0007, Accuracy: 152407/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0038, Accuracy: 37331/40024 (93%)\n",
      "\n",
      "epoch 63/ 100\n",
      "Time Taken:  99.48650574684143\n",
      "Training set: Average loss: 0.0007, Accuracy: 152535/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 37354/40024 (93%)\n",
      "\n",
      "epoch 64/ 100\n",
      "Time Taken:  98.87995457649231\n",
      "Training set: Average loss: 0.0007, Accuracy: 152659/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 37368/40024 (93%)\n",
      "\n",
      "epoch 65/ 100\n",
      "Time Taken:  99.28532338142395\n",
      "Training set: Average loss: 0.0007, Accuracy: 152768/160094 (95%)\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 37387/40024 (93%)\n",
      "\n",
      "epoch 66/ 100\n",
      "Time Taken:  99.70170164108276\n",
      "Training set: Average loss: 0.0007, Accuracy: 152892/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0037, Accuracy: 37410/40024 (93%)\n",
      "\n",
      "epoch 67/ 100\n",
      "Time Taken:  99.81480503082275\n",
      "Training set: Average loss: 0.0007, Accuracy: 152985/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 37430/40024 (94%)\n",
      "\n",
      "epoch 68/ 100\n",
      "Time Taken:  99.3373703956604\n",
      "Training set: Average loss: 0.0007, Accuracy: 153078/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 37451/40024 (94%)\n",
      "\n",
      "epoch 69/ 100\n",
      "Time Taken:  99.5535671710968\n",
      "Training set: Average loss: 0.0007, Accuracy: 153179/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 37471/40024 (94%)\n",
      "\n",
      "epoch 70/ 100\n",
      "Time Taken:  98.67877125740051\n",
      "Training set: Average loss: 0.0007, Accuracy: 153266/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 37482/40024 (94%)\n",
      "\n",
      "epoch 71/ 100\n",
      "Time Taken:  99.32716417312622\n",
      "Training set: Average loss: 0.0007, Accuracy: 153364/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 37493/40024 (94%)\n",
      "\n",
      "epoch 72/ 100\n",
      "Time Taken:  95.5339617729187\n",
      "Training set: Average loss: 0.0006, Accuracy: 153428/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 37516/40024 (94%)\n",
      "\n",
      "epoch 73/ 100\n",
      "Time Taken:  95.42537641525269\n",
      "Training set: Average loss: 0.0006, Accuracy: 153515/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 37534/40024 (94%)\n",
      "\n",
      "epoch 74/ 100\n",
      "Time Taken:  96.85492968559265\n",
      "Training set: Average loss: 0.0006, Accuracy: 153599/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 37551/40024 (94%)\n",
      "\n",
      "epoch 75/ 100\n",
      "Time Taken:  96.37757992744446\n",
      "Training set: Average loss: 0.0006, Accuracy: 153688/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 37571/40024 (94%)\n",
      "\n",
      "epoch 76/ 100\n",
      "Time Taken:  95.65737104415894\n",
      "Training set: Average loss: 0.0006, Accuracy: 153762/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 37589/40024 (94%)\n",
      "\n",
      "epoch 77/ 100\n",
      "Time Taken:  96.01463413238525\n",
      "Training set: Average loss: 0.0006, Accuracy: 153859/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0034, Accuracy: 37602/40024 (94%)\n",
      "\n",
      "epoch 78/ 100\n",
      "Time Taken:  95.95098376274109\n",
      "Training set: Average loss: 0.0006, Accuracy: 153927/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0034, Accuracy: 37621/40024 (94%)\n",
      "\n",
      "epoch 79/ 100\n",
      "Time Taken:  96.47030878067017\n",
      "Training set: Average loss: 0.0006, Accuracy: 153995/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0034, Accuracy: 37635/40024 (94%)\n",
      "\n",
      "epoch 80/ 100\n",
      "Time Taken:  95.57130455970764\n",
      "Training set: Average loss: 0.0006, Accuracy: 154080/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0034, Accuracy: 37652/40024 (94%)\n",
      "\n",
      "epoch 81/ 100\n",
      "Time Taken:  95.16349530220032\n",
      "Training set: Average loss: 0.0006, Accuracy: 154159/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0034, Accuracy: 37659/40024 (94%)\n",
      "\n",
      "epoch 82/ 100\n",
      "Time Taken:  98.71068596839905\n",
      "Training set: Average loss: 0.0006, Accuracy: 154238/160094 (96%)\n",
      "\n",
      "Test set: Average loss: 0.0034, Accuracy: 37676/40024 (94%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-ac64a71bc801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtrain_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxinput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myoutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m#xinput = xinput.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#youtput = youtput.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model = SimpleNetwork(131695, 100, 3)\n",
    "end = time.time()\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = LEARNING_RATE)\n",
    "n_total_steps = len(training_generator)\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train the network by filling in this block of code\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    start = time.time()\n",
    "    for i,(xinput,youtput) in enumerate(training_generator):\n",
    "        #xinput = xinput.to(device)\n",
    "        #youtput = youtput.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xinput.float())\n",
    "        loss = criterion(outputs,youtput.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #Record Train Accuracy\n",
    "        train_loss += loss.item()\n",
    "        train_pred = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        train_correct += train_pred.eq(youtput.view_as(train_pred)).sum().item()\n",
    "    train_loss /= len(training_generator.dataset)\n",
    "    #Report Validation Accuracy\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testing_generator:\n",
    "            output = model(data.float())\n",
    "            test_loss += criterion(output, target.long()).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(testing_generator.dataset)\n",
    "        end = time.time()\n",
    "        print(f'epoch {epoch+1}/ {EPOCHS}')\n",
    "        print('Time Taken: ',end - start)\n",
    "        print('Training set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            train_loss, train_correct, len(training_generator.dataset),\n",
    "            100. * train_correct / len(training_generator.dataset)))\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(testing_generator.dataset),\n",
    "            100. * correct / len(testing_generator.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'sentimentModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNetwork(\n",
       "  (l1): Linear(in_features=131695, out_features=100, bias=True)\n",
       "  (reLu): ReLU()\n",
       "  (l2): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentModel = SimpleNetwork(131695, 100, 3)\n",
    "sentimentModel.load_state_dict(torch.load('sentimentModel.pth'))\n",
    "sentimentModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0033, Accuracy: 37684/40024 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "        for data, target in testing_generator:\n",
    "            output = sentimentModel(data.float())\n",
    "            test_loss += criterion(output, target.long()).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(testing_generator.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(testing_generator.dataset),\n",
    "            100. * correct / len(testing_generator.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a comment:\n",
      "I hate you\n",
      "ðŸ˜”\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "sentiment = {0:emoji.emojize(\":pensive_face:\"),\n",
    "             1:emoji.emojize(\":neutral_face:\"),\n",
    "             2:emoji.emojize(\":smiling_face_with_smiling_eyes:\")}\n",
    "comment = np.array([input(\"Please enter a comment:\\n\")])\n",
    "#print('That comment is ',clf.predict(vect.transform(comment)[0]))\n",
    "prediction = sentimentModel(torch.tensor((vect.transform(comment)[0].astype('int16').toarray())).float())\n",
    "# Predicted class value using argmax\n",
    "print(sentiment[prediction.detach().numpy().argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
